{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 1: Training with multiple GPUs from scratch\n",
    "\n",
    "Adapted from https://gluon.mxnet.io/chapter07_distributed-learning/multiple-gpus-scratch.html\n",
    "\n",
    "This tutorial shows how we can increase performance by distributing training across multiple GPUs.\n",
    "\n",
    "* This tutorial requires at least 2 GPUs.\n",
    "* An NVIDIA driver must be installed on the machine.\n",
    "* MXNet must be installed with gpu capabilities (https://www.nvidia.com/en-us/data-center/gpu-accelerated-applications/mxnet/)\n",
    "* Tested on a Amazon SageMaker notebook p3.8xlarge instance. \n",
    "\n",
    "![](../img/multi-gpu.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start by checking the configuration of the GPUs installed on your machine:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Jul 10 14:40:47 2018       \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| NVIDIA-SMI 384.111                Driver Version: 384.111                   |\r\n",
      "|-------------------------------+----------------------+----------------------+\r\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n",
      "|===============================+======================+======================|\r\n",
      "|   0  Tesla V100-SXM2...  On   | 00000000:00:1B.0 Off |                    0 |\r\n",
      "| N/A   49C    P0    53W / 300W |      0MiB / 16152MiB |      0%      Default |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "|   1  Tesla V100-SXM2...  On   | 00000000:00:1C.0 Off |                    0 |\r\n",
      "| N/A   48C    P0    50W / 300W |      0MiB / 16152MiB |      0%      Default |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "|   2  Tesla V100-SXM2...  On   | 00000000:00:1D.0 Off |                    0 |\r\n",
      "| N/A   44C    P0    40W / 300W |      0MiB / 16152MiB |      0%      Default |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "|   3  Tesla V100-SXM2...  On   | 00000000:00:1E.0 Off |                    0 |\r\n",
      "| N/A   44C    P0    37W / 300W |      0MiB / 16152MiB |      0%      Default |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "                                                                               \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| Processes:                                                       GPU Memory |\r\n",
      "|  GPU       PID   Type   Process name                             Usage      |\r\n",
      "|=============================================================================|\r\n",
      "|  No running processes found                                                 |\r\n",
      "+-----------------------------------------------------------------------------+\r\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to use all of the GPUs on together for the purpose of significantly speeding up training (in terms of wall clock). \n",
    "Remember that CPUs and GPUs each can have multiple cores. \n",
    "CPUs on a laptop might have 2 or 4 cores, and on a server might have up to 16 or 32 cores. \n",
    "GPUs tend to have many more cores - an NVIDIA Tesla V100 GPU has 5120 - but run at slower clock speeds. \n",
    "Exploiting the parallelism across the GPU cores is how GPUs get their speed advantage in the first place. \n",
    "\n",
    "As compared to the single CPU or single GPU setting where all the cores are typically used by default,\n",
    "parallelism across devices is a little more complicated.\n",
    "That's because most layers of a neural network can only run on a single device. \n",
    "So, in order to parallelize across devices, we need to do a little extra.\n",
    "Therefore, we need to do some additional work to partition a workload across multiple GPUs. \n",
    "This can be done in a few ways.\n",
    "\n",
    "## Data Parallelism\n",
    "\n",
    "For deep learning, data parallelism is by far the most widely used approach for partitioning workloads. \n",
    "It works like this: Assume that we have *k* GPUs. We split the examples in a data batch into *k* parts,\n",
    "and send each part to a different GPUs which then computes the gradient that part of the batch. \n",
    "Finally, we collect the gradients from each of the GPUs and sum them together before updating the weights.\n",
    "\n",
    "The following pseudo-code shows how to train one data batch on *k* GPUs.\n",
    "\n",
    "\n",
    "```\n",
    "def train_batch(data, k):\n",
    "    split data into k parts\n",
    "    for i = 1, ..., k:  # run in parallel\n",
    "        compute grad_i w.r.t. weight_i using data_i on the i-th GPU\n",
    "    grad = grad_1 + ... + grad_k\n",
    "    for i = 1, ..., k:  # run in parallel\n",
    "        copy grad to i-th GPU\n",
    "        update weight_i by using grad\n",
    "```\n",
    "\n",
    "Next we will present how to implement this algorithm from scratch.\n",
    "\n",
    "\n",
    "## Automatic Parallelization\n",
    "\n",
    "We first demonstrate how to run workloads in parallel. \n",
    "Writing parallel code in Python in non-trivial, but fortunately, \n",
    "MXNet is able to automatically parallelize the workloads. \n",
    "Two technologies help to achieve this goal.\n",
    "\n",
    "First, workloads, such as `nd.dot` are pushed into the backend engine for *lazy evaluation*. \n",
    "That is, Python merely pushes the workload `nd.dot` and returns immediately \n",
    "without waiting for the computation to be finished. \n",
    "We keep pushing until the results need to be copied out from MXNet, \n",
    "such as `print(x)` or are converted into numpy by `x.asnumpy()`. \n",
    "At that time, the Python thread is blocked until the results are ready."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mxnet as mx\n",
    "from mxnet import nd\n",
    "from time import time\n",
    "from mxnet import gpu\n",
    "from mxnet import cpu\n",
    "from mxnet import gluon\n",
    "from mxnet.test_utils import get_mnist\n",
    "from mxnet.io import NDArrayIter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1: Lazy Evaluation.\n",
    "\n",
    "Create a random NDArray and multiply it with itself:\n",
    "\n",
    "Fill in the lines in the next cell to:\n",
    "1. Create a NDArray of size 2000x2000 called x.\n",
    "2. Multiply the array by itself and assign the result to y.\n",
    "3. Trigger the execution by converting to a numpy array. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== workloads are pushed into the backend engine ===\n",
      "0.000944 sec\n",
      "=== workloads are finished ===\n",
      "0.057008 sec\n",
      "(2000, 2000)\n"
     ]
    }
   ],
   "source": [
    "start = time()\n",
    "# 2 lines, create the ndarray x, multiply it by itself and assign to y\n",
    "x = nd.random_uniform(shape=(2000,2000))\n",
    "y = nd.dot(x, x)\n",
    "print('=== workloads are pushed into the backend engine ===\\n%f sec' % (time() - start))\n",
    "# 1 line, convert to numpy and assign to z\n",
    "z = y.asnumpy()\n",
    "print('=== workloads are finished ===\\n%f sec' % (time() - start))\n",
    "print(z.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Expected Output\n",
    "```\n",
    "=== workloads are pushed into the backend engine ===\n",
    "0.002730 sec\n",
    "=== workloads are finished ===\n",
    "0.048943 sec\n",
    "(2000, 2000)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Second, MXNet depends on a powerful scheduling algorithm that analyzes the dependencies of the pushed workloads.\n",
    "This scheduler checks to see if two workloads are independent of each other.\n",
    "If they are, then the engine may run them in parallel.\n",
    "If a workload depend on results that have not yet been computed, it will be made to wait until its inputs are ready.\n",
    "\n",
    "For example, if we call three operators:\n",
    "\n",
    "```\n",
    "a = nd.random_uniform(...)\n",
    "b = nd.random_uniform(...)\n",
    "c = a + b\n",
    "```\n",
    "\n",
    "Then the computation for `a` and `b` may run in parallel, \n",
    "while `c` cannot be computed until both `a` and `b` are ready. \n",
    "\n",
    "The following code shows that the engine effectively parallelizes the `dot` operations on two GPUs:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2: Parallel Computation\n",
    "\n",
    "Create an array stored on a GPU, run a complex job in series and in parallel:\n",
    "\n",
    "1. Create a random NDArray of size 4000x4000 on gpu(0)\n",
    "2. Create a copy of the array on gpu(1)\n",
    "3. Run the workload in series and parallel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Run on GPU 0 and 1 in sequential ===\n",
      "time: 2.020221 sec\n",
      "=== Run on GPU 0 and 1 in parallel ===\n",
      "time: 0.103789 sec\n"
     ]
    }
   ],
   "source": [
    "def run(x):\n",
    "    \"\"\"push 10 matrix-matrix multiplications\"\"\"\n",
    "    return [nd.dot(x,x) for i in range(10)]\n",
    "\n",
    "def wait(x):\n",
    "    \"\"\"explicitly wait until all results are ready\"\"\"\n",
    "    for y in x:\n",
    "        y.wait_to_read()\n",
    "\n",
    "# 1 line create the 4000x4000 random array on gpu(0)\n",
    "x0 = nd.random_uniform(shape=(4000, 4000), ctx=gpu(0))\n",
    "# 1 line copy the array to gpu(1) and set as x1\n",
    "x1 = x0.copyto(gpu(1))\n",
    "\n",
    "print('=== Run on GPU 0 and 1 in sequential ===')\n",
    "start = time()\n",
    "wait(run(x0))\n",
    "wait(run(x1))\n",
    "print('time: %f sec' %(time() - start))\n",
    "\n",
    "print('=== Run on GPU 0 and 1 in parallel ===')\n",
    "start = time()\n",
    "y0 = run(x0)\n",
    "y1 = run(x1)\n",
    "wait(y0)\n",
    "wait(y1)\n",
    "print('time: %f sec' %(time() - start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Expected Output\n",
    "```\n",
    "=== Run on GPU 0 and 1 in sequential ===\n",
    "time: 2.022742 sec\n",
    "=== Run on GPU 0 and 1 in parallel ===\n",
    "time: 0.099376 sec\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3: Parallel Computation 2\n",
    "\n",
    "Run on gpu and copy back to the CPU sequentially and then in parallel\n",
    "\n",
    "1. Run the workload on the created array xo and assign to y0\n",
    "2. Wait on the results of the computation\n",
    "3. Copy the results to the cpu\n",
    "4. Wait for the copy to complete\n",
    "\n",
    "5. Run the workload on the created array xo\n",
    "6. Copy the results to the cpu in parallel\n",
    "7. Wait for the parallel operations to complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Run on GPU 0 and then copy results to CPU in sequential ===\n",
      "0.5026967525482178\n",
      "=== Run and copy in parallel ===\n",
      "0.42870044708251953\n"
     ]
    }
   ],
   "source": [
    "def copy(x, ctx):\n",
    "    \"\"\"copy data to a device\"\"\"\n",
    "    return [y.copyto(ctx) for y in x]\n",
    "\n",
    "print('=== Run on GPU 0 and then copy results to CPU in sequential ===')\n",
    "start = time()\n",
    "# 1 line run the workload on array xo and assign to y\n",
    "y0 = run(x0)\n",
    "# 1 line wait for the computation to complete\n",
    "wait(y0)\n",
    "# 1 line copy y0 back to the cpu\n",
    "z0 = copy(y0, cpu())\n",
    "wait(z0)\n",
    "print(time() - start)\n",
    "\n",
    "print('=== Run and copy in parallel ===')\n",
    "start = time()\n",
    "y0 = run(x0)\n",
    "z0 = copy(y0, cpu())\n",
    "wait(z0)\n",
    "print(time() - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Expected Output\n",
    "```\n",
    "=== Run on GPU 0 and then copy results to CPU in sequential ===\n",
    "0.5133333206176758\n",
    "=== Run and copy in parallel ===\n",
    "0.4283921718597412\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 4: Define CNN model and updater\n",
    "\n",
    "We will use the convolutional neural networks and plain SGD introduced in cnn-scratch as an example workload.\n",
    "\n",
    "The task is to initialise the weights, define the first layer of the network, and set the loss function.\n",
    "\n",
    "1. Create the parameter array for the first layer of shape=(20,1,3,3), use random_normal() with a scale of 0.01\n",
    "2. Create the bias parameter array of length 20, filled with zeros\n",
    "3. Create the first convolutional layer with a 3x3 kernel and 20 filters.\n",
    "4. Define the loss function using SoftmaxCrossEntropyLoss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize parameters\n",
    "scale = .01\n",
    "# 1 line: Create the parameter array W1 filled by random_normal() with shape (20,1,3,3), multiply by scale (0.01)\n",
    "W1 = nd.random_normal(shape=(20,1,3,3))*scale\n",
    "# 1 line: Initialise the bias parameter array with zeros, length 20\n",
    "b1 = nd.zeros(shape=20)\n",
    "W2 = nd.random_normal(shape=(50,20,5,5))*scale\n",
    "b2 = nd.zeros(shape=50)\n",
    "W3 = nd.random_normal(shape=(800,128))*scale\n",
    "b3 = nd.zeros(shape=128)\n",
    "W4 = nd.random_normal(shape=(128,10))*scale\n",
    "b4 = nd.zeros(shape=10)\n",
    "params = [W1, b1, W2, b2, W3, b3, W4, b4]\n",
    "\n",
    "# network and loss\n",
    "def lenet(X, params):\n",
    "    # 1 line: Create the first layer using nd.Convolution, 3x3 kernel, and 20 filters\n",
    "    h1_conv = nd.Convolution(data=X, weight=params[0], bias=params[1], kernel=(3,3), num_filter=20)\n",
    "    h1_activation = nd.relu(h1_conv)\n",
    "    h1 = nd.Pooling(data=h1_activation, pool_type=\"max\", kernel=(2,2), stride=(2,2))\n",
    "    # second conv\n",
    "    h2_conv = nd.Convolution(data=h1, weight=params[2], bias=params[3], kernel=(5,5), num_filter=50)\n",
    "    h2_activation = nd.relu(h2_conv)\n",
    "    h2 = nd.Pooling(data=h2_activation, pool_type=\"max\", kernel=(2,2), stride=(2,2))\n",
    "    h2 = nd.flatten(h2)\n",
    "    # first fullc\n",
    "    h3_linear = nd.dot(h2, params[4]) + params[5]\n",
    "    h3 = nd.relu(h3_linear)\n",
    "    # second fullc    \n",
    "    yhat = nd.dot(h3, params[6]) + params[7]    \n",
    "    return yhat\n",
    "\n",
    "# 1 line: Define the loss function using SoftmaxCrossEntropyLoss\n",
    "loss = gluon.loss.SoftmaxCrossEntropyLoss()\n",
    "\n",
    "# plain SGD\n",
    "def SGD(params, lr):\n",
    "    for p in params:\n",
    "        p[:] = p - lr * p.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 5: Define a Function to Copy Parameters to a GPU\n",
    "\n",
    "Create a function to copy the given parameters to a given gpu and initialise the gradients. \n",
    "\n",
    "1. In the list comprehension, copy each parameter array to the given gpu.\n",
    "2. Use the ndarray.attach_grad() function to initialise the gradients for each parameter array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== copy b1 to GPU(0) ===\n",
      "weight = \n",
      "[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.]\n",
      "<NDArray 20 @gpu(0)>\n",
      "grad = \n",
      "[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.]\n",
      "<NDArray 20 @gpu(0)>\n"
     ]
    }
   ],
   "source": [
    "def get_params(params, ctx):\n",
    "    # 1 element: copy each parameter to the given gpu\n",
    "    new_params = [p.copyto(ctx) for p in params]\n",
    "    for p in new_params:\n",
    "        # 1 line: initialise the gradients.\n",
    "        p.attach_grad()\n",
    "    return new_params\n",
    "\n",
    "new_params = get_params(params, gpu(0))\n",
    "print('=== copy b1 to GPU(0) ===\\nweight = {}\\ngrad = {}'.format(\n",
    "    new_params[1], new_params[1].grad))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Expected Output\n",
    "```\n",
    "=== copy b1 to GPU(0) ===\n",
    "weight = \n",
    "[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
    "  0.  0.]\n",
    "<NDArray 20 @gpu(0)>\n",
    "grad = \n",
    "[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
    "  0.  0.]\n",
    "<NDArray 20 @gpu(0)>\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Task 6: Create a function to sum and broadcast the results\n",
    "\n",
    "Given a list of data that spans multiple GPUs, we then define a function to sum the gradients from all GPUs \n",
    "and broadcast the result to each GPU. \n",
    "\n",
    "1. Copy each array in the input to the same GPU as the first element.\n",
    "2. In the same line sum all the arrays in the input\n",
    "3. Copy the summed result to all GPUs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== before allreduce ===\n",
      " [\n",
      "[[ 1.  1.]]\n",
      "<NDArray 1x2 @gpu(0)>, \n",
      "[[ 2.  2.]]\n",
      "<NDArray 1x2 @gpu(1)>]\n",
      "\n",
      "=== after allreduce ===\n",
      " [\n",
      "[[ 3.  3.]]\n",
      "<NDArray 1x2 @gpu(0)>, \n",
      "[[ 3.  3.]]\n",
      "<NDArray 1x2 @gpu(1)>]\n"
     ]
    }
   ],
   "source": [
    "def allreduce(data):\n",
    "    # sum on data[0].context, and then broadcast\n",
    "    for i in range(1, len(data)):\n",
    "        # 1 line: Copy each array in the input to data[0].context and sum all arrays\n",
    "        data[0][:] += data[i].copyto(data[0].context)\n",
    "    for i in range(1, len(data)):\n",
    "        # 1 line: Copy this sum back to all the GPUs\n",
    "        data[0].copyto(data[i])        \n",
    "\n",
    "data = [nd.ones((1,2), ctx=gpu(i))*(i+1) for i in range(2)]\n",
    "print(\"=== before allreduce ===\\n {}\".format(data))\n",
    "allreduce(data)\n",
    "print(\"\\n=== after allreduce ===\\n {}\".format(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Expected Output\n",
    "```\n",
    "=== before allreduce ===\n",
    " [\n",
    "[[ 1.  1.]]\n",
    "<NDArray 1x2 @gpu(0)>, \n",
    "[[ 2.  2.]]\n",
    "<NDArray 1x2 @gpu(1)>]\n",
    "\n",
    "=== after allreduce ===\n",
    " [\n",
    "[[ 3.  3.]]\n",
    "<NDArray 1x2 @gpu(0)>, \n",
    "[[ 3.  3.]]\n",
    "<NDArray 1x2 @gpu(1)>]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 7: Split into Batches and Copy to GPUs\n",
    "\n",
    "Given a data batch, we define a function that splits this batch and copies each part into the corresponding GPU.\n",
    "\n",
    "1. In the list comprehension copy each data batch to the correct GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== original data ===\n",
      "[[  0.   1.   2.   3.]\n",
      " [  4.   5.   6.   7.]\n",
      " [  8.   9.  10.  11.]\n",
      " [ 12.  13.  14.  15.]]\n",
      "<NDArray 4x4 @cpu(0)>\n",
      "\n",
      "=== splitted into [gpu(0), gpu(1)] ===\n",
      "[[ 0.  1.  2.  3.]\n",
      " [ 4.  5.  6.  7.]]\n",
      "<NDArray 2x4 @gpu(0)>\n",
      "\n",
      "[[  8.   9.  10.  11.]\n",
      " [ 12.  13.  14.  15.]]\n",
      "<NDArray 2x4 @gpu(1)>\n"
     ]
    }
   ],
   "source": [
    "def split_and_load(data, ctx):\n",
    "    n, k = data.shape[0], len(ctx)\n",
    "    assert (n//k)*k == n, '# examples is not divided by # devices'\n",
    "    idx = list(range(0, n+1, n//k))\n",
    "    # 1 element: Copy each data batch to the correct GPU\n",
    "    return [data[idx[i]:idx[i+1]].as_in_context(ctx[i]) for i in range(k)]\n",
    "\n",
    "batch = nd.arange(16).reshape((4,4))\n",
    "print('=== original data ==={}'.format(batch))\n",
    "ctx = [gpu(0), gpu(1)]\n",
    "splitted = split_and_load(batch, ctx)\n",
    "print('\\n=== splitted into {} ==={}\\n{}'.format(ctx, splitted[0], splitted[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 8: Create a Function to Train on a Single Batch\n",
    "\n",
    "To train the batch the following is performed:\n",
    "\n",
    "1. Split the batch up equally and load onto the GPUs using our split_and_load(data,ctx) function.\n",
    "2. Run forward and backward passes on each GPU.\n",
    "3. Sum all the gradients on each GPU and broadcast back to each GPU.\n",
    "4. Update the parameters using the gradients.\n",
    "\n",
    "A lot of that is already done, the task is to:\n",
    "\n",
    "1. Split and load the batch data and labels onto the GPUs.\n",
    "2. Calculate the gradients on each GPU.\n",
    "3. Use the allreduce function to sum the gradients on all GPUs and broadcast the sum back."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_batch(batch, params, ctx, lr):\n",
    "    # split the data batch and load them on GPUs\n",
    "    # 2 lines: use the split_and_load function to split and copy data and label onto the GPUs\n",
    "    data = split_and_load(batch.data[0], ctx)\n",
    "    label = split_and_load(batch.label[0], ctx)\n",
    "    # run forward on each GPU\n",
    "    with mx.autograd.record():\n",
    "        losses = [loss(lenet(X, W), Y) \n",
    "                  for X, Y, W in zip(data, label, params)]\n",
    "    # run backward on each gpu\n",
    "    for l in losses:\n",
    "        # 1 line calculate the gradients on each GPU\n",
    "        l.backward()\n",
    "    # aggregate gradient over GPUs\n",
    "    for i in range(len(params[0])):\n",
    "        # 1-3 lines: use the allreduce function to sum the gradients on all GPUs and\n",
    "        # broadcast the sum back to all GPUs.\n",
    "        # Hint: pass a list to allreduce of params[j][i] for each j in the number of GPUs\n",
    "        allreduce([params[c][i].grad for c in range(len(ctx))])\n",
    "    # update parameters with SGD on each GPU\n",
    "    for p in params:\n",
    "        SGD(p, lr/batch.data[0].shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For inference, we simply let it run on the first GPU. We leave a data parallelism implementation as an exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def valid_batch(batch, params, ctx):\n",
    "    data = batch.data[0].as_in_context(ctx[0])\n",
    "    pred = nd.argmax(lenet(data, params[0]), axis=1)\n",
    "    return nd.sum(pred == batch.label[0].as_in_context(ctx[0])).asscalar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Task 9: Put it All Together to Train and Validate on MNIST\n",
    "\n",
    "It is ready to put together to train and validate. It is run on the MNIST dataset.\n",
    "\n",
    "1. Load the MNIST dataset\n",
    "2. Copy the parameters to the GPUs\n",
    "3. Iterate over the dataset 5 times, 5 epochs\n",
    "4. Split into batches\n",
    "5. Use the previous function to train on a batch\n",
    "6. Check the accuracy on the validation set after each epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def run(num_gpus, batch_size, lr):    \n",
    "    # the list of GPUs will be used\n",
    "    ctx = [gpu(i) for i in range(num_gpus)]\n",
    "    print('Running on {}'.format(ctx))\n",
    "    \n",
    "    # data iterator\n",
    "    mnist = get_mnist()\n",
    "    train_data = NDArrayIter(mnist[\"train_data\"], mnist[\"train_label\"], batch_size)\n",
    "    valid_data = NDArrayIter(mnist[\"test_data\"], mnist[\"test_label\"], batch_size)\n",
    "    print('Batch size is {}'.format(batch_size))\n",
    "    \n",
    "    # copy parameters to all GPUs\n",
    "    dev_params = [get_params(params, c) for c in ctx]\n",
    "    for epoch in range(5):\n",
    "        # train\n",
    "        start = time()\n",
    "        train_data.reset()\n",
    "        for batch in train_data:\n",
    "            train_batch(batch, dev_params, ctx, lr)\n",
    "        nd.waitall()  # wait all computations are finished to benchmark the time\n",
    "        print('Epoch %d, training time = %.1f sec'%(epoch, time()-start))\n",
    "        \n",
    "        # validating\n",
    "        valid_data.reset()\n",
    "        correct, num = 0.0, 0.0\n",
    "        for batch in valid_data:\n",
    "            correct += valid_batch(batch, dev_params, ctx)\n",
    "            num += batch.data[0].shape[0]                \n",
    "        print('         validation accuracy = %.4f'%(correct/num))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First run on a single GPU with batch size 64."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on [gpu(0)]\n",
      "Batch size is 64\n",
      "Epoch 0, training time = 2.7 sec\n",
      "         validation accuracy = 0.9622\n",
      "Epoch 1, training time = 2.7 sec\n",
      "         validation accuracy = 0.9763\n",
      "Epoch 2, training time = 2.7 sec\n",
      "         validation accuracy = 0.9807\n",
      "Epoch 3, training time = 2.7 sec\n",
      "         validation accuracy = 0.9830\n",
      "Epoch 4, training time = 2.7 sec\n",
      "         validation accuracy = 0.9850\n"
     ]
    }
   ],
   "source": [
    "run(1, 64, 0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Often it is necessary to increase the batch size when running on multiple GPUs, so each one works on a large enough sample.\n",
    "* Increasing the batch size can decrease the convergence rate, the learning rate can be increased or more epochs can be run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on [gpu(0), gpu(1)]\n",
      "Batch size is 64\n",
      "Epoch 0, training time = 6.6 sec\n",
      "         validation accuracy = 0.9642\n",
      "Epoch 1, training time = 6.6 sec\n",
      "         validation accuracy = 0.9775\n",
      "Epoch 2, training time = 6.6 sec\n",
      "         validation accuracy = 0.9812\n",
      "Epoch 3, training time = 6.6 sec\n",
      "         validation accuracy = 0.9830\n",
      "Epoch 4, training time = 6.6 sec\n",
      "         validation accuracy = 0.9833\n"
     ]
    }
   ],
   "source": [
    "run(2, 64, 0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Congratulations!\n",
    "\n",
    "You have learnt how to implement data parallelism across multiple GPUs using MXNet from scratch. Some issues were uncovered with more communication overhead than computational speedup. The next lab covers the same using Gluon which is much simpler to implement and further optimised. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next\n",
    "[Training with multiple GPUs with gluon](../chapter07_distributed-learning/multiple-gpus-gluon.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For whinges or inquiries, [open an issue on  GitHub.](https://github.com/edenduthie/scalable_mxnet)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_mxnet_p36",
   "language": "python",
   "name": "conda_mxnet_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
